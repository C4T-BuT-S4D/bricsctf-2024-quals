// @generated
// This file is @generated by prost-build.
/// Training configuration for the Random Forest algorithm.
///
/// Next ID: 26
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct DecisionTreeTrainingConfig {
    // Basic parameters.

    /// Maximum depth of the tree. max_depth=1 means that all trees will be roots.
    /// If max_depth=-1, the depth of the tree is not limited.
    #[prost(int32, optional, tag="1", default="16")]
    pub max_depth: ::core::option::Option<i32>,
    /// Minimum number of examples in a node.
    #[prost(int32, optional, tag="2", default="5")]
    pub min_examples: ::core::option::Option<i32>,
    // Advanced parameters.

    /// Whether to check the "min_examples" constraint in the split search (i.e.
    /// splits leading to one child having less than "min_examples" examples are
    /// considered invalid) or before the split search (i.e. a node can be
    /// derived only if it contains more than "min_examples" examples). If false,
    /// there can be nodes with less than "min_examples" training examples.
    #[prost(bool, optional, tag="3", default="true")]
    pub in_split_min_examples_check: ::core::option::Option<bool>,
    /// Whether to store the full distribution (e.g. the distribution of all
    /// the possible label values in case of classification) or only the top label
    /// (e.g. the most representative class). This information is used for model
    /// interpretation as well as in case of "winner_take_all_inference=false".
    /// In the worst case, this information can account for a significant part of
    /// the model size.
    #[prost(bool, optional, tag="4", default="true")]
    pub store_detailed_label_distribution: ::core::option::Option<bool>,
    /// INFO: Use "pure_serving_model" instead of
    /// "keep_non_leaf_label_distribution". "pure_serving_model" is more general,
    /// works in more situations, and removes more unused data from the model.
    ///
    /// Whether to keep the node value (i.e. the distribution of the labels of the
    /// training examples) of non-leaf nodes. This information is not used during
    /// serving, however it can be used for model interpretation as well as hyper
    /// parameter tuning. In the worst case, this can account for half of the model
    /// size.
    ///
    /// keep_non_leaf_label_distribution=false is not compatible with monotonic
    /// constraints.
    #[prost(bool, optional, tag="5", default="true")]
    pub keep_non_leaf_label_distribution: ::core::option::Option<bool>,
    #[prost(enumeration="decision_tree_training_config::MissingValuePolicy", optional, tag="7", default="GlobalImputation")]
    pub missing_value_policy: ::core::option::Option<i32>,
    /// If true, the tree training evaluates conditions of the type "X is NA" i.e.
    /// "X is missing".
    #[prost(bool, optional, tag="8", default="false")]
    pub allow_na_conditions: ::core::option::Option<bool>,
    #[prost(message, optional, tag="12")]
    pub categorical_set_greedy_forward: ::core::option::Option<GreedyForwardCategoricalSet>,
    #[prost(message, optional, tag="15")]
    pub numerical_split: ::core::option::Option<NumericalSplit>,
    /// Options related to the learning of categorical splits.
    #[prost(message, optional, tag="16")]
    pub categorical: ::core::option::Option<Categorical>,
    /// Generate an error (if true) or a warning (if false) when the statistics
    /// exported by splitters don't match the observed statistics.
    ///
    /// This fields is used in the unit tests.
    #[prost(bool, optional, tag="18", default="false")]
    pub internal_error_on_wrong_splitter_statistics: ::core::option::Option<bool>,
    /// Uplift specific hyper-parameters.
    #[prost(message, optional, tag="22")]
    pub uplift: ::core::option::Option<decision_tree_training_config::Uplift>,
    // If set, the decision tree is trained to be honest.
    //
    // In honest trees, different training examples are used to infer the
    // structure and the leaf values. This regularization technique trades
    // examples for bias estimates. It might increase or reduce the quality of the
    // model.
    //
    // See "Generalized Random Forests", Athey et al. In this paper, Honest tree
    // are trained with the Random Forest algorithm with a sampling without
    // replacement.

    #[prost(message, optional, tag="24")]
    pub honest: ::core::option::Option<decision_tree_training_config::Honest>,
    /// Internal knobs of the algorithm that don't impact the final model.
    #[prost(message, optional, tag="21")]
    pub internal: ::core::option::Option<decision_tree_training_config::Internal>,
    #[prost(oneof="decision_tree_training_config::ControlNumCandidateAttributes", tags="6, 17")]
    pub control_num_candidate_attributes: ::core::option::Option<decision_tree_training_config::ControlNumCandidateAttributes>,
    /// How to grow the tree.
    #[prost(oneof="decision_tree_training_config::GrowingStrategy", tags="13, 14")]
    pub growing_strategy: ::core::option::Option<decision_tree_training_config::GrowingStrategy>,
    /// What structure of split to consider.
    #[prost(oneof="decision_tree_training_config::SplitAxis", tags="19, 20, 25")]
    pub split_axis: ::core::option::Option<decision_tree_training_config::SplitAxis>,
}
/// Nested message and enum types in `DecisionTreeTrainingConfig`.
pub mod decision_tree_training_config {
    /// See "split_axis".
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct AxisAlignedSplit {
    }
    /// See "split_axis".
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct SparseObliqueSplit {
        /// Controls of the number of random projections to test at each node.
        ///
        /// Increasing this value very likely improves the quality of the model,
        /// drastically increases the training time, and doe not impact the
        /// inference time.
        ///
        /// Oblique splits try out max(p^num_projections_exponent,
        /// max_num_projections) random projections for choosing a split, where p is
        /// the number of numerical features. Therefore, increasing this
        /// `num_projections_exponent` and possibly `max_num_projections` may improve
        /// model quality, but will also significantly increase training time.
        ///
        /// Note that the complexity of (classic) Random Forests is roughly
        /// proportional to `num_projections_exponent=0.5`, since it considers
        /// sqrt(num_features) for a split. The complexity of (classic) GBDT is
        /// roughly proportional to `num_projections_exponent=1`, since it considers
        /// all features for a split.
        ///
        /// The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
        /// recommends values in \[1/4, 2\].
        #[prost(float, optional, tag="1", default="2")]
        pub num_projections_exponent: ::core::option::Option<f32>,
        /// Maximum number of projections (applied after the
        /// "num_projections_exponent").
        ///
        /// Oblique splits try out max(p^num_projections_exponent,
        /// max_num_projections) random projections for choosing a split, where p is
        /// the number of numerical features. Increasing "max_num_projections"
        /// increases the training time but not the inference time. In late stage
        /// model development, if every bit of accuracy if important, increase this
        /// value.
        ///
        /// The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
        /// does not define this hyperparameter.
        #[prost(int32, optional, tag="2", default="6000")]
        pub max_num_projections: ::core::option::Option<i32>,
        /// Minimum number of projections.
        ///
        /// In a dataset with very few numerical features, increasing this parameter
        /// might improve model quality.
        ///
        /// The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
        /// does not define this hyperparameter.
        #[prost(int32, optional, tag="6", default="1")]
        pub min_num_projections: ::core::option::Option<i32>,
        /// Density of the projections as an exponent of the number of features.
        /// Independently for each projection, each feature has a probability
        /// "projection_density_factor / num_features" to be considered in the
        /// projection.
        ///
        /// The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
        /// calls this parameter `lambda` and recommends values in \[1, 5\].
        ///
        /// Increasing this value increases training and inference time (on average).
        /// This value is best tuned for each dataset.
        #[prost(float, optional, tag="3", default="2")]
        pub projection_density_factor: ::core::option::Option<f32>,
        /// If true, the weight will be sampled in {-1,1} (default in  "Sparse
        /// Projection Oblique Random Forests" (Tomita et al, 2020)). If false, the
        /// weight will be sampled in \[-1,1\].
        #[prost(bool, optional, tag="4", default="true")]
        pub binary_weight: ::core::option::Option<bool>,
        /// Normalization applied on the features, before applying the sparse oblique
        /// projections.
        #[prost(enumeration="sparse_oblique_split::Normalization", optional, tag="5", default="None")]
        pub normalization: ::core::option::Option<i32>,
    }
    /// Nested message and enum types in `SparseObliqueSplit`.
    pub mod sparse_oblique_split {
        #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
        #[repr(i32)]
        pub enum Normalization {
            /// No normalization. Logic used in the  "Sparse Projection Oblique Random
            /// Forests" (Tomita et al, 2020).
            None = 0,
            /// Normalize the feature by the estimated standard deviation on the entire
            /// train dataset. Also known as Z-Score normalization.
            StandardDeviation = 1,
            /// Normalize the feature by the range (i.e. max-min) estimated on the
            /// entire train dataset.
            MinMax = 2,
        }
        impl Normalization {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Normalization::None => "NONE",
                    Normalization::StandardDeviation => "STANDARD_DEVIATION",
                    Normalization::MinMax => "MIN_MAX",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "NONE" => Some(Self::None),
                    "STANDARD_DEVIATION" => Some(Self::StandardDeviation),
                    "MIN_MAX" => Some(Self::MinMax),
                    _ => None,
                }
            }
        }
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct MhldObliqueSplit {
        /// Maximum number of attributes in the projection. Increasing this value
        /// increases the training time. Decreasing this value acts as a
        /// regularization. The value should be in \[2, num_numerical_features\]. If
        /// the value is above num_numerical_features, the value is capped to
        /// num_numerical_features. The value 1 is allowed but results in ordinary
        /// (non-oblique) splits
        #[prost(int32, optional, tag="1", default="4")]
        pub max_num_attributes: ::core::option::Option<i32>,
        /// If true, applies the attribute sampling in "num_candidate_attributes" and
        /// "num_candidate_attributes_ratio". If false, all attributes are tested.
        #[prost(bool, optional, tag="2", default="false")]
        pub sample_attributes: ::core::option::Option<bool>,
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Uplift {
        /// Minimum number of examples per treatment in a node. Only used for uplift
        /// models.
        #[prost(int32, optional, tag="1", default="5")]
        pub min_examples_in_treatment: ::core::option::Option<i32>,
        #[prost(enumeration="uplift::SplitScore", optional, tag="2", default="KullbackLeibler")]
        pub split_score: ::core::option::Option<i32>,
        #[prost(enumeration="uplift::EmptyBucketOrdering", optional, tag="3", default="ParentTreatmentOutcome")]
        pub empty_bucket_ordering: ::core::option::Option<i32>,
    }
    /// Nested message and enum types in `Uplift`.
    pub mod uplift {
        /// Splitter score i.e. score optimized by the splitters. Changing the
        /// splitter score will impact the trained model.
        ///
        /// The following scores are introduced in "Decision trees for uplift
        /// modeling with single and multiple treatments", Rzepakowski et al.
        ///
        /// Notation:
        ///   p: probability of the positive outcome (categorical outcome) or average
        ///     value of the outcome (numerical outcome) in the treatment group.
        ///   q: probability / average value in the control group.
        #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
        #[repr(i32)]
        pub enum SplitScore {
            /// Score: - p log (p/q)
            /// Categorical outcome only.
            KullbackLeibler = 0,
            /// Score: (p-q)^2
            /// Categorical outcome only.
            /// TODO: Add numerical outcome.
            EuclideanDistance = 1,
            /// Score: (p-q)^2/q
            /// Categorical outcome only.
            ChiSquared = 2,
            /// Conservative estimate (lower bound) of the euclidean distance.
            ConservativeEuclideanDistance = 3,
        }
        impl SplitScore {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    SplitScore::KullbackLeibler => "KULLBACK_LEIBLER",
                    SplitScore::EuclideanDistance => "EUCLIDEAN_DISTANCE",
                    SplitScore::ChiSquared => "CHI_SQUARED",
                    SplitScore::ConservativeEuclideanDistance => "CONSERVATIVE_EUCLIDEAN_DISTANCE",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "KULLBACK_LEIBLER" => Some(Self::KullbackLeibler),
                    "EUCLIDEAN_DISTANCE" => Some(Self::EuclideanDistance),
                    "CHI_SQUARED" => Some(Self::ChiSquared),
                    "CONSERVATIVE_EUCLIDEAN_DISTANCE" => Some(Self::ConservativeEuclideanDistance),
                    _ => None,
                }
            }
        }
        /// How to order buckets having no values for one of the treatments.
        /// This parameter is used exclusively for the bucket sorting during the
        /// generation of some of the candidate splits. For example, for categorical
        /// features with the CART splitter
        #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
        #[repr(i32)]
        pub enum EmptyBucketOrdering {
            /// Uses the treatment conditional mean outcome of the parent node.
            ParentTreatmentOutcome = 0,
            /// Uses the mean outcome of the parent node.
            ParentOutcome = 1,
        }
        impl EmptyBucketOrdering {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    EmptyBucketOrdering::ParentTreatmentOutcome => "PARENT_TREATMENT_OUTCOME",
                    EmptyBucketOrdering::ParentOutcome => "PARENT_OUTCOME",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "PARENT_TREATMENT_OUTCOME" => Some(Self::ParentTreatmentOutcome),
                    "PARENT_OUTCOME" => Some(Self::ParentOutcome),
                    _ => None,
                }
            }
        }
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Honest {
        /// Ratio of examples used to set the leaf values.
        #[prost(float, optional, tag="1", default="0.5")]
        pub ratio_leaf_examples: ::core::option::Option<f32>,
        /// If true, a new random separation is generated for each tree.
        /// If false, the same separation is used for all the trees (for examples,
        /// in a Gradient Boosted Trees containing multiple trees).
        #[prost(bool, optional, tag="2", default="false")]
        pub fixed_separation: ::core::option::Option<bool>,
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Internal {
        #[prost(enumeration="internal::SortingStrategy", optional, tag="21", default="Auto")]
        pub sorting_strategy: ::core::option::Option<i32>,
        /// If set, ensures that the effective strategy is
        /// "ensure_effective_strategy". "ensure_effective_strategy" is only used in
        /// unit test when the sorting strategy is not manually set i.e.
        /// sorting_strategy = AUTO.
        #[prost(enumeration="internal::SortingStrategy", optional, tag="1")]
        pub ensure_effective_sorting_strategy: ::core::option::Option<i32>,
        /// If false, the score of a hessian split is:
        ///    score \approx \sum_{children} sum_grad^2/sum_hessian
        ///
        /// If true, the score of a hessian split is:
        ///    score \approx (\sum_{children} sum_grad^2/sum_hessian) -
        ///      sum_grad_parent^2/sum_hessian_parent.
        ///
        /// This flag has two effects:
        /// - The absolute value of the score is different (e.g. when looking at the
        ///    variable importance).
        /// - When growing the tree with global optimization, the structure of the
        ///    tree might differ (however there is not impact on the structure when
        ///    using the divide and conquer strategy).
        ///
        /// YDF used implicitly hessian_split_score_subtract_parent=false. XGBoost
        /// uses hessian_split_score_subtract_parent=true but the paper is explicit
        /// that this is just a possible solution. Both versions make sense (and
        /// produce similar results). Another possible version would be subtracting
        /// the parent gradient before the square.
        ///
        /// An experiment was conducted on 68 datasets, 10 folds CV, and 3 times
        /// repetitions to evaluate the effect of this flags. Both methods produce
        /// close models. However, in average accuracy, average auc and average
        /// rank, the "false" method is better than the "true" one by a small but
        /// visible margin.
        #[prost(bool, optional, tag="22", default="false")]
        pub hessian_split_score_subtract_parent: ::core::option::Option<bool>,
        /// If true, partially checks monotonic constraints of trees after training.
        /// This option is used by unit testing. That is, check that the value of a
        /// positive node is greater than the value of a generative note (in case of
        /// increasing monotonic constraint). If false and if a monotonic constraint
        /// is not satisfied, the monotonic constraint is manually enforced.
        ///
        /// The current checking implementation might detect as non-monotonic trees
        /// that are in fact monotonic (e.g. false positive). However, with the
        /// current algorithm used to create monotonic constraints, this checking
        /// algorithm cannot create false positives.
        #[prost(bool, optional, tag="23", default="false")]
        pub check_monotonic_constraints: ::core::option::Option<bool>,
    }
    /// Nested message and enum types in `Internal`.
    pub mod internal {
        /// How the computation of sorted values (non discretized numerical values)
        /// are obtained.
        #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
        #[repr(i32)]
        pub enum SortingStrategy {
            /// Values are sorted within each node.
            InNode = 0,
            /// Values are pre-sorted into an index to speed-up training. The index
            /// will be automatically ignored When using the index is slower than
            /// sorting in-node or if the algorithm does not benefit from pre-sorting.
            /// This method can increase significantly the amount of memory required
            /// for training.
            Presorted = 1,
            /// Always use the presorted index, even if the result would be slower.
            /// For testing only.
            ForcePresorted = 2,
            /// Select automatically the best method (The quickest method that does not
            /// consume excessive RAM).
            Auto = 3,
        }
        impl SortingStrategy {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    SortingStrategy::InNode => "IN_NODE",
                    SortingStrategy::Presorted => "PRESORTED",
                    SortingStrategy::ForcePresorted => "FORCE_PRESORTED",
                    SortingStrategy::Auto => "AUTO",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "IN_NODE" => Some(Self::InNode),
                    "PRESORTED" => Some(Self::Presorted),
                    "FORCE_PRESORTED" => Some(Self::ForcePresorted),
                    "AUTO" => Some(Self::Auto),
                    _ => None,
                }
            }
        }
    }
    /// Method used to handle missing attribute values.
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
    #[repr(i32)]
    pub enum MissingValuePolicy {
        /// Missing attribute values are imputed, with the mean (in case of numerical
        /// attribute) or the most-frequent-item (in case of categorical attribute)
        /// computed on the entire dataset (i.e. the information contained in the
        /// data spec).
        GlobalImputation = 0,
        /// Missing attribute values are imputed with the mean (numerical attribute)
        /// or most-frequent-item (in the case of categorical attribute) evaluated on
        /// the training examples in the current node.
        LocalImputation = 1,
        /// Missing attribute values are imputed from randomly sampled values from
        /// the training examples in the current node. This method was proposed by
        /// Clinic et al. in "Random Survival Forests"
        /// (<https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043>).
        RandomLocalImputation = 2,
    }
    impl MissingValuePolicy {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                MissingValuePolicy::GlobalImputation => "GLOBAL_IMPUTATION",
                MissingValuePolicy::LocalImputation => "LOCAL_IMPUTATION",
                MissingValuePolicy::RandomLocalImputation => "RANDOM_LOCAL_IMPUTATION",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "GLOBAL_IMPUTATION" => Some(Self::GlobalImputation),
                "LOCAL_IMPUTATION" => Some(Self::LocalImputation),
                "RANDOM_LOCAL_IMPUTATION" => Some(Self::RandomLocalImputation),
                _ => None,
            }
        }
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum ControlNumCandidateAttributes {
        /// Number of unique valid attributes tested for each node. An attribute is
        /// valid if it has at least a valid split. If
        /// "num_candidate_attributes=0" the value is set to the classical default
        /// value for Random Forest: "sqrt(number of input attributes)" in case of
        /// classification and "number of input attributes/3" in case of regression.
        /// If "num_candidate_attributes=-1", all the attributes are tested.
        #[prost(int32, tag="6")]
        NumCandidateAttributes(i32),
        /// If set, replaces "num_candidate_attributes" with the
        /// "number_of_input_features x num_candidate_attributes_ratio". The possible
        /// values are between ]0, and 1] as well as -1. If not set or equal to -1,
        /// the "num_candidate_attributes" is used.
        #[prost(float, tag="17")]
        NumCandidateAttributesRatio(f32),
    }
    /// How to grow the tree.
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum GrowingStrategy {
        /// \[Default strategy\] Each node is split independently of the other nodes.
        /// In other words, as long as a node satisfy the splits constraints (e.g.
        /// maximum depth, minimum number of observations), the node will be split.
        ///
        /// This is the "classical" way to grow decision trees.
        #[prost(message, tag="13")]
        GrowingStrategyLocal(super::GrowingStrategyLocalBest),
        /// The node with the best loss reduction among all the nodes of the tree is
        /// selected for splitting.
        ///
        /// This method is also called "best first" or "leaf-wise growth".
        /// See "Best-first decision tree learning", Shi
        /// (<http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.2862&rep=rep1&type=pdf>)
        /// and "Additive logistic regression : A statistical view of boosting",
        /// Friedman et al. (<https://projecteuclid.org/euclid.aos/1016218223>) for
        /// more details.
        #[prost(message, tag="14")]
        GrowingStrategyBestFirstGlobal(super::GrowingStrategyGlobalBest),
    }
    /// What structure of split to consider.
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum SplitAxis {
        /// Axis aligned splits (i.e. one condition at a time). This is the
        /// "classical" way to train a tree. Default value.
        #[prost(message, tag="19")]
        AxisAlignedSplit(AxisAlignedSplit),
        /// Sparse oblique splits (i.e. splits one a small number of features) from
        /// "Sparse Projection Oblique Random Forests", Tomita et al., 2020. These
        /// splits are tested iif. "sparse_oblique_split" is set.
        #[prost(message, tag="20")]
        SparseObliqueSplit(SparseObliqueSplit),
        /// Oblique splits from "Classification Based on Multivariate Contrast
        /// Patterns" by  Canete-Sifuentes et al.
        #[prost(message, tag="25")]
        MhldObliqueSplit(MhldObliqueSplit),
    }
}
/// How to find numerical splits.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct NumericalSplit {
    #[prost(enumeration="numerical_split::Type", optional, tag="1", default="Exact")]
    pub r#type: ::core::option::Option<i32>,
    /// Number of candidate thresholds. Ignored for EXACT.
    /// Default:
    /// HISTOGRAM_RANDOM => 1
    /// HISTOGRAM_EQUAL_WIDTH => 255
    #[prost(int32, optional, tag="2")]
    pub num_candidates: ::core::option::Option<i32>,
}
/// Nested message and enum types in `NumericalSplit`.
pub mod numerical_split {
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
    #[repr(i32)]
    pub enum Type {
        /// Original/CART splitting. Slow but gives good (small, high accuracy)
        /// models. Equivalent to XGBoost Exact.
        Exact = 0,
        // Note: The histogram splitters are implemented as simply as possible,
        // and mainly for comparison purpose. The speed would be significantly
        // improved (especially for deep trees), but the learned  models would
        // remain the same.

        /// Select candidate splits randomly between the min and max values.
        /// Similar to the ExtraTrees algorithm:
        /// <https://link.springer.com/content/pdf/10.1007%2Fs10994-006-6226-1.pdf>
        HistogramRandom = 1,
        /// Select the candidate splits uniformly (in the feature space) between the
        /// min and max value.
        HistogramEqualWidth = 2,
    }
    impl Type {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Type::Exact => "EXACT",
                Type::HistogramRandom => "HISTOGRAM_RANDOM",
                Type::HistogramEqualWidth => "HISTOGRAM_EQUAL_WIDTH",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "EXACT" => Some(Self::Exact),
                "HISTOGRAM_RANDOM" => Some(Self::HistogramRandom),
                "HISTOGRAM_EQUAL_WIDTH" => Some(Self::HistogramEqualWidth),
                _ => None,
            }
        }
    }
}
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct GreedyForwardCategoricalSet {
    /// Probability for a categorical value to be a candidate for the positive set
    /// in the extraction of a categorical set split. The sampling is applied
    /// once per node (i.e. not at every step of the greedy optimization).
    #[prost(float, optional, tag="1", default="0.1")]
    pub sampling: ::core::option::Option<f32>,
    /// Maximum number of items (prior to the sampling). If more items are
    /// available, the least frequent items are ignored. Changing this value is
    /// similar to change the "max_vocab_count" before loading the dataset, with
    /// the following exception: With "max_vocab_count", all the remaining items
    /// are grouped in a special Out-of-vocabulary item. With "max_num_items", this
    /// is not the case.
    #[prost(int32, optional, tag="2", default="-1")]
    pub max_num_items: ::core::option::Option<i32>,
    /// Minimum number of occurrences of an item to be considered.
    #[prost(int32, optional, tag="3", default="1")]
    pub min_item_frequency: ::core::option::Option<i32>,
    /// Maximum number of items selected in the condition.
    /// Note: max_selected_items=1 is equivalent to one-hot encoding.
    #[prost(int32, optional, tag="4", default="-1")]
    pub max_selected_items: ::core::option::Option<i32>,
}
/// How to handle categorical input features.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct Categorical {
    /// If the dictionary size of the attribute is greater or equal to
    /// "arity_limit_for_random", the "random" algorithm is be used (instead of
    /// the algorithm specified in "algorithm");
    #[prost(int32, optional, tag="4", default="300")]
    pub arity_limit_for_random: ::core::option::Option<i32>,
    #[prost(oneof="categorical::Algorithm", tags="1, 2, 3")]
    pub algorithm: ::core::option::Option<categorical::Algorithm>,
}
/// Nested message and enum types in `Categorical`.
pub mod categorical {
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Cart {
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct OneHot {
        /// Sampling of the item tested. 1. means that all the items will be tested.
        #[prost(float, optional, tag="1", default="1")]
        pub sampling: ::core::option::Option<f32>,
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Random {
        /// Controls the number of random splits to evaluated.
        ///
        /// The effective number of splits is "min(max_num_trials, num_trial_offset +
        /// {vocab size}^num_trial_exponent"), with "vocab size" the number of unique
        /// categorical values in the node.
        #[prost(float, optional, tag="1", default="2")]
        pub num_trial_exponent: ::core::option::Option<f32>,
        #[prost(float, optional, tag="2", default="32")]
        pub num_trial_offset: ::core::option::Option<f32>,
        /// Maximum number of candidates.
        #[prost(int32, optional, tag="3", default="5000")]
        pub max_num_trials: ::core::option::Option<i32>,
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum Algorithm {
        /// CART algorithm (default).
        ///
        /// Find the a categorical split of the form "value \in mask". The solution
        /// is exact for binary classification, regression and ranking. It is
        /// approximated for multi-class classification.
        ///
        /// This is a good first algorithm to use. In case of overfitting (very small
        /// dataset, large dictionary), the "random" algorithm is a good alternative.
        #[prost(message, tag="1")]
        Cart(Cart),
        /// One-hot encoding.
        ///
        /// Find the optimal categorical split of the form "attribute == param".
        /// This method is similar (but more efficient) than converting converting
        /// each possible categorical value into a boolean feature.
        ///
        /// This method is available for comparison purpose and generally performs
        /// worst than other alternatives.
        #[prost(message, tag="2")]
        OneHot(OneHot),
        /// Best splits among a set of random candidate.
        ///
        /// Find the a categorical split of the form "value \in mask" using a random
        /// search. This solution can be seen as an approximation of the CART
        /// algorithm.
        ///
        /// This method is a strong alternative to CART.
        ///
        /// This algorithm is inspired from section "5.1 Categorical Variables" of
        /// "Random Forest", 2001
        /// (<https://www.stat.berkeley.edu/users/breiman/randomforest2001.pdf>).
        ///
        #[prost(message, tag="3")]
        Random(Random),
    }
}
/// Specifies the local best growing strategy. No extra configuration needed.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct GrowingStrategyLocalBest {
}
/// Specifies the global best growing strategy.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct GrowingStrategyGlobalBest {
    /// Maximum number of nodes in the tree. Set to "-1" to disable this limit.
    #[prost(int32, optional, tag="1", default="31")]
    pub max_num_nodes: ::core::option::Option<i32>,
}
/// Statistics about the label values used to operate a splitter algorithm.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LabelStatistics {
    #[prost(int64, optional, tag="1")]
    pub num_examples: ::core::option::Option<i64>,
    #[prost(oneof="label_statistics::Type", tags="2, 3, 4")]
    pub r#type: ::core::option::Option<label_statistics::Type>,
}
/// Nested message and enum types in `LabelStatistics`.
pub mod label_statistics {
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Classification {
        #[prost(message, optional, tag="1")]
        pub labels: ::core::option::Option<super::super::utils::IntegerDistributionDouble>,
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Regression {
        #[prost(message, optional, tag="1")]
        pub labels: ::core::option::Option<super::super::utils::NormalDistributionDouble>,
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct RegressionWithHessian {
        #[prost(message, optional, tag="1")]
        pub labels: ::core::option::Option<super::super::utils::NormalDistributionDouble>,
        #[prost(double, optional, tag="2")]
        pub sum_hessian: ::core::option::Option<f64>,
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Type {
        #[prost(message, tag="2")]
        Classification(Classification),
        #[prost(message, tag="3")]
        Regression(Regression),
        #[prost(message, tag="4")]
        RegressionWithHessian(RegressionWithHessian),
    }
}
/// Output of a node in a classification tree.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NodeClassifierOutput {
    /// Next ID: 3
    /// Label value.
    #[prost(int32, optional, tag="1")]
    pub top_value: ::core::option::Option<i32>,
    /// Distribution of label values. The most frequent value is "top_value".
    #[prost(message, optional, tag="2")]
    pub distribution: ::core::option::Option<super::utils::IntegerDistributionDouble>,
}
/// Output of a node in a regression tree.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct NodeRegressorOutput {
    /// Next ID: 6
    /// Label value.
    #[prost(float, optional, tag="1")]
    pub top_value: ::core::option::Option<f32>,
    /// Distribution of label values. The mean is "top_value".
    #[prost(message, optional, tag="2")]
    pub distribution: ::core::option::Option<super::utils::NormalDistributionDouble>,
    /// Statistics of hessian splits.
    #[prost(double, optional, tag="3")]
    pub sum_gradients: ::core::option::Option<f64>,
    #[prost(double, optional, tag="4")]
    pub sum_hessians: ::core::option::Option<f64>,
    #[prost(double, optional, tag="5")]
    pub sum_weights: ::core::option::Option<f64>,
}
/// Output of a node in an uplift tree with either binary categorical or
/// numerical outcome.
///
/// The fields have the same definition as the fields in the message
/// "UpliftCategoricalLabelDistribution".
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NodeUpliftOutput {
    /// Weighted number of examples.
    #[prost(double, optional, tag="1")]
    pub sum_weights: ::core::option::Option<f64>,
    /// Currently, the code only support binary categorical or regressive outcomes.
    #[prost(double, repeated, tag="2")]
    pub sum_weights_per_treatment: ::prost::alloc::vec::Vec<f64>,
    /// Number of examples for each outcome (major) and each treatment (minor).
    /// In the case of categorical outcome, exclude the zero outcome. For example,
    /// in case of binary treatment, "sum_weights_per_treatment_and_outcome"
    /// contains one value for each treatment. In the case of numerical outcome,
    /// "sum_weights_per_treatment_and_outcome" is the weighted sum of the
    /// outcomes.
    ///
    /// Currently, the code only supports binary categorical or regressive outcome.
    #[prost(double, repeated, tag="3")]
    pub sum_weights_per_treatment_and_outcome: ::prost::alloc::vec::Vec<f64>,
    /// treatment_effect\[i\] is the effect of the "i+1"-th treatment (categorical
    /// value i+2) compared to the control group (0-th treatment; categorical
    /// value = 1). The treatment out-of-vocabulary item (value = 0) is not taken
    /// into account.
    #[prost(float, repeated, tag="4")]
    pub treatment_effect: ::prost::alloc::vec::Vec<f32>,
    /// Number of examples in each treatment. Not weighted.
    #[prost(int64, repeated, tag="5")]
    pub num_examples_per_treatment: ::prost::alloc::vec::Vec<i64>,
}
/// Output of a node in an anomaliy detection tree.
///
/// Next ID: 2
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct NodeAnomalyDetectionOutput {
    /// Number of examples that reached this node.
    #[prost(int64, optional, tag="1")]
    pub num_examples_without_weight: ::core::option::Option<i64>,
}
/// The sub-messages of "ConditionParams" are the different types of condition
/// that can be attached to a node.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Condition {
    /// Type of condition.
    #[prost(oneof="condition::Type", tags="1, 2, 3, 4, 5, 6, 7")]
    pub r#type: ::core::option::Option<condition::Type>,
}
/// Nested message and enum types in `Condition`.
pub mod condition {
    /// Next ID: 6
    /// Condition of the type: value == NA (i.e. missing).
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Na {
    }
    /// Condition of the type: value == True.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct TrueValue {
    }
    /// Condition of the type: value >= threshold.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Higher {
        /// \[Required\]
        #[prost(float, optional, tag="1")]
        pub threshold: ::core::option::Option<f32>,
    }
    /// Condition of the type: (value \intersect elements) != empty_set.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ContainsVector {
        /// Next ID: 2
        #[prost(int32, repeated, tag="1")]
        pub elements: ::prost::alloc::vec::Vec<i32>,
    }
    /// Condition of the type: (value \intersect elements) != empty_set where
    /// elements is stored as a bitmap over the possible values.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ContainsBitmap {
        /// Next ID: 2
        /// \[Required\]
        #[prost(bytes="vec", optional, tag="1")]
        pub elements_bitmap: ::core::option::Option<::prost::alloc::vec::Vec<u8>>,
    }
    /// Condition of the type: indexed_value >= indexed_threshold.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct DiscretizedHigher {
        /// \[Required\]
        #[prost(int32, optional, tag="1")]
        pub threshold: ::core::option::Option<i32>,
    }
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Oblique {
        /// True iff. \sum_i examples\[attribute_i\] * weight_i >= threshold.
        /// The "attribute" field in "NodeCondition" should be one of the
        /// "attributes" in this message. If any of the attributes is missing, the
        /// conditions evaluates to missing and returns "na_value".
        #[prost(int32, repeated, tag="1")]
        pub attributes: ::prost::alloc::vec::Vec<i32>,
        #[prost(float, repeated, tag="2")]
        pub weights: ::prost::alloc::vec::Vec<f32>,
        #[prost(float, optional, tag="3")]
        pub threshold: ::core::option::Option<f32>,
        /// If set, "na_replacements" defines the replacement values for missing
        /// attributes. For example, if attributes = \[3,5\], where attribute 3 is
        /// available and attribute 5 is missing. The condition will be evaluated
        /// with attribute 3's value and the replacement value na_replacements\[1\].
        ///
        /// If not set, in case of a missing value of any of the attributes, the
        /// condition evaluates to "na_value".
        #[prost(float, repeated, tag="4")]
        pub na_replacements: ::prost::alloc::vec::Vec<f32>,
    }
    /// Type of condition.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Type {
        #[prost(message, tag="1")]
        NaCondition(Na),
        #[prost(message, tag="2")]
        HigherCondition(Higher),
        #[prost(message, tag="3")]
        TrueValueCondition(TrueValue),
        #[prost(message, tag="4")]
        ContainsCondition(ContainsVector),
        #[prost(message, tag="5")]
        ContainsBitmapCondition(ContainsBitmap),
        #[prost(message, tag="6")]
        DiscretizedHigherCondition(DiscretizedHigher),
        /// Make sure to update "kNumConditionTypes" in "decision_tree.h"
        /// accordingly.
        #[prost(message, tag="7")]
        ObliqueCondition(Oblique),
    }
}
/// Binary condition attached to a non-leaf node.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NodeCondition {
    /// Next ID: 9
    /// Evaluation value of this condition in case of a NA (i.e. missing) value.
    #[prost(bool, optional, tag="1")]
    pub na_value: ::core::option::Option<bool>,
    /// Attribute on which the condition applies.
    #[prost(int32, optional, tag="2")]
    pub attribute: ::core::option::Option<i32>,
    /// If the condition is not set, this node is a leaf node.
    #[prost(message, optional, tag="3")]
    pub condition: ::core::option::Option<Condition>,
    /// Number of examples (non-weighted) that reached this node during training.
    #[prost(int64, optional, tag="4")]
    pub num_training_examples_without_weight: ::core::option::Option<i64>,
    /// Number of examples (weighted) that reached this node during training.
    #[prost(double, optional, tag="5")]
    pub num_training_examples_with_weight: ::core::option::Option<f64>,
    /// Score attached to the split.
    #[prost(float, optional, tag="6", default="0")]
    pub split_score: ::core::option::Option<f32>,
    /// Number of positive examples (non-weighted) that reached this node during
    /// training.
    #[prost(int64, optional, tag="7")]
    pub num_pos_training_examples_without_weight: ::core::option::Option<i64>,
    /// Number of positive examples (weighted) that reached this node during
    /// training.
    #[prost(double, optional, tag="8")]
    pub num_pos_training_examples_with_weight: ::core::option::Option<f64>,
}
/// Node in a decision tree (without the information about the children).
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Node {
    /// Branching condition to the children. If not specified, this node is a leaf.
    #[prost(message, optional, tag="3")]
    pub condition: ::core::option::Option<NodeCondition>,
    /// Number of examples (non-weighted) that reached this node during
    /// training. Warning: Contrary to what the name suggest, this is not the count
    /// of examples branched to the positive child.
    #[prost(int64, optional, tag="4")]
    pub num_pos_training_examples_without_weight: ::core::option::Option<i64>,
    /// Next ID: 7
    /// Label value. Might be unspecified for non-leaf nodes.
    #[prost(oneof="node::Output", tags="1, 2, 5, 6")]
    pub output: ::core::option::Option<node::Output>,
}
/// Nested message and enum types in `Node`.
pub mod node {
    /// Next ID: 7
    /// Label value. Might be unspecified for non-leaf nodes.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Output {
        #[prost(message, tag="1")]
        Classifier(super::NodeClassifierOutput),
        #[prost(message, tag="2")]
        Regressor(super::NodeRegressorOutput),
        #[prost(message, tag="5")]
        Uplift(super::NodeUpliftOutput),
        #[prost(message, tag="6")]
        AnomalyDetection(super::NodeAnomalyDetectionOutput),
    }
}
// @@protoc_insertion_point(module)
