// @generated
// This file is @generated by prost-build.
/// Training configuration for the Random Forest algorithm.
///
/// Next ID: 17
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RandomForestTrainingConfig {
    // Basic parameters.

    /// Number of trees in the random forest.
    #[prost(int32, optional, tag="1", default="300")]
    pub num_trees: ::core::option::Option<i32>,
    /// Decision tree specific parameters.
    #[prost(message, optional, tag="2")]
    pub decision_tree: ::core::option::Option<super::decision_tree::DecisionTreeTrainingConfig>,
    // Advanced parameters.

    /// Whether the vote of individual trees are distributions or winner-take-all.
    ///
    /// With winner_take_all_inference=true, each tree cast a vote for a single
    /// label value. With winner_take_all_inference=false, each tree cast a
    /// weighted vote for each label values.
    ///
    /// The original random forest implementation uses
    /// winner_take_all_inference=true (default value). However,
    /// "winner_take_all_inference=false" often leads to better results and smaller
    /// models.
    #[prost(bool, optional, tag="3", default="true")]
    pub winner_take_all_inference: ::core::option::Option<bool>,
    /// Computes and report the OOB performances of the model during training. The
    /// added computing cost is relatively small.
    #[prost(bool, optional, tag="4", default="true")]
    pub compute_oob_performances: ::core::option::Option<bool>,
    /// Computes the importance of each variable (i.e. each input feature) during
    /// training. Computing the variable importance is expensive and can
    /// significantly slow down the training.
    #[prost(bool, optional, tag="5", default="false")]
    pub compute_oob_variable_importances: ::core::option::Option<bool>,
    /// Number of time the dataset is shuffled for each tree when computing the
    /// variable importances. Increasing this number can increase significantly the
    /// training time (if "compute_oob_variable_importances:true") and increase the
    /// stability of the oob variable importance metrics.
    #[prost(int32, optional, tag="6", default="1")]
    pub num_oob_variable_importances_permutations: ::core::option::Option<i32>,
    /// The Out-of-bag evaluation is computed if one of the condition is true:
    ///    - This is the last tree of the model.
    ///    - The last OOB was computed more than
    ///      "oob_evaluation_interval_in_seconds" ago.
    ///    - This last OOB was computed more than "oob_evaluation_interval_in_trees"
    ///      trees ago.
    #[prost(float, optional, tag="7", default="10")]
    pub oob_evaluation_interval_in_seconds: ::core::option::Option<f32>,
    #[prost(float, optional, tag="14", default="10")]
    pub oob_evaluation_interval_in_trees: ::core::option::Option<f32>,
    /// If true, each tree is trained on a separate dataset sampled with
    /// replacement from the original dataset. If false, all the trees are trained
    /// on the same dataset.
    ///
    /// Note: If bootstrap_training_dataset:false, OOB metrics are not available.
    ///
    /// bootstrap_training_dataset:true is the default value for Random Forest.
    /// bootstrap_training_dataset:false can be used to simulate related decision
    /// forest algorithms (e.g. "Extremely randomized trees"
    /// <https://link.springer.com/content/pdf/10.1007%2Fs10994-006-6226-1.pdf>).
    #[prost(bool, optional, tag="8", default="true")]
    pub bootstrap_training_dataset: ::core::option::Option<bool>,
    /// If true, the training examples are sampled with replacement. If false, the
    /// training samples are sampled without replacement. Only used when
    /// "bootstrap_training_dataset=true".
    ///
    /// If false (sampling without replacement) and if "bootstrap_size_ratio=1"
    /// (default), all the examples are used to train all the trees (you probably
    /// do not want that).
    #[prost(bool, optional, tag="15", default="true")]
    pub sampling_with_replacement: ::core::option::Option<bool>,
    /// Number of example in each bootstrap expressed as a ratio of the training
    /// dataset size.
    #[prost(float, optional, tag="9", default="1")]
    pub bootstrap_size_ratio: ::core::option::Option<f32>,
    /// If true, the "bootstrap_size_ratio" parameter will be adapted dynamically
    /// such that the "num_trees" will be trained in the
    /// "maximum_training_duration" time. "bootstrap_size_ratio" can only be
    /// reduced i.e. enabling this feature can only reduce the training time.
    #[prost(bool, optional, tag="11", default="false")]
    pub adapt_bootstrap_size_ratio_for_maximum_training_duration: ::core::option::Option<bool>,
    /// Maximum impact of the
    /// "adapt_bootstrap_size_ratio_for_maximum_training_duration"
    /// parameter.
    #[prost(float, optional, tag="12", default="0.01")]
    pub min_adapted_subsample: ::core::option::Option<f32>,
    /// Total maximum of nodes in the model. If specified, and if the total number
    /// of nodes is exceeded, the training stops and the forest is truncated.
    #[prost(int64, optional, tag="13", default="-1")]
    pub total_max_num_nodes: ::core::option::Option<i64>,
    /// If set, and if "compute_oob_performances" is true, export the out-of-bag
    /// predictions of the model on the training dataset in the file specified by
    /// "export_oob_prediction_path". Note that "export_oob_prediction_path" is a
    /// typed-path e.g. a path with a format prefix.
    ///
    /// The writer implementation of the format should be linked in the binary. For
    /// example, to export the predictions to the csv or tfrecord+tfe format, you
    /// need to make sure that dataset:csv_example_writer or
    /// dataset:tf_example_io_tfrecord are respectively linked.
    ///
    /// Example:
    ///    export_oob_prediction_path = "csv:/tmp/oob_predictions.csv"
    #[prost(string, optional, tag="16")]
    pub export_oob_prediction_path: ::core::option::Option<::prost::alloc::string::String>,
    /// Fields used for low level and/or internal API. In most cases, the user
    /// should not care about this field.
    #[prost(message, optional, tag="10")]
    pub internal: ::core::option::Option<Internal>,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Internal {
    /// Individual random seeds user to train the trees. Is specified, the number
    /// of seeds should be equal to "num_trees".
    #[prost(int64, repeated, packed="false", tag="1")]
    pub individual_tree_seeds: ::prost::alloc::vec::Vec<i64>,
}
/// Header for the random forest model.
///
/// Next ID: 7
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Header {
    /// Number of shards used to store the nodes.
    #[prost(int32, optional, tag="1")]
    pub num_node_shards: ::core::option::Option<i32>,
    /// Number of trees.
    #[prost(int64, optional, tag="2")]
    pub num_trees: ::core::option::Option<i64>,
    /// Whether the vote of individual trees are distributions or winner-take-all.
    #[prost(bool, optional, tag="3", default="true")]
    pub winner_take_all_inference: ::core::option::Option<bool>,
    /// Evaluation of the model, on the out-of-bag examples, during the training.
    #[prost(message, repeated, tag="4")]
    pub out_of_bag_evaluations: ::prost::alloc::vec::Vec<OutOfBagTrainingEvaluations>,
    /// Variable importance measures.
    #[prost(message, repeated, tag="5")]
    pub mean_decrease_in_accuracy: ::prost::alloc::vec::Vec<super::model::VariableImportance>,
    #[prost(message, repeated, tag="6")]
    pub mean_increase_in_rmse: ::prost::alloc::vec::Vec<super::model::VariableImportance>,
    /// Container used to store the trees' nodes.
    #[prost(string, optional, tag="7", default="TFE_RECORDIO")]
    pub node_format: ::core::option::Option<::prost::alloc::string::String>,
    /// Number of nodes trained and then pruned during the training.
    /// The classical random forest learning algorithm does not prune nodes.
    #[prost(int64, optional, tag="8")]
    pub num_pruned_nodes: ::core::option::Option<i64>,
}
/// Next ID: 3
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct OutOfBagTrainingEvaluations {
    /// Number of trees available in the model when evaluated.
    #[prost(int32, optional, tag="1")]
    pub number_of_trees: ::core::option::Option<i32>,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RandomForestSerializedModel {
    #[prost(message, optional, tag="1")]
    pub header: ::core::option::Option<Header>,
}
// @@protoc_insertion_point(module)
