// @generated
// This file is @generated by prost-build.
/// Generic hyper parameters of a learner.
///
/// Learner hyper parameters are normally provided through the "TrainingConfig"
/// proto extended by each learner. The "Generic hyper parameters" (the following
/// message) is a parallel solution to specify the hyper parameters of a learner
/// using a list of key-values.
///
/// The "Generic hyper parameters" are designed for the interfacing with
/// hyper-parameter optimization algorithms, while the "TrainingConfig" proto is
/// designed for direct user input. For this reason, the generic hyper parameters
/// are not guaranteed to be as expressive as the "TrainingConfig".
/// However, the default values of the "Generic hyper parameters" are guaranteed
/// to be equivalent to the default value of the training config.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenericHyperParameters {
    #[prost(message, repeated, tag="1")]
    pub fields: ::prost::alloc::vec::Vec<generic_hyper_parameters::Field>,
    /// Unique id of the parameters.
    /// Might be missing if the parameters are generated by a user, or by a
    /// AbstractOptimizer that does not require ids.
    #[prost(int64, optional, tag="2")]
    pub id: ::core::option::Option<i64>,
}
/// Nested message and enum types in `GenericHyperParameters`.
pub mod generic_hyper_parameters {
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Field {
        /// Hyper parameter name. Should match the "name" of the hyper parameter
        /// specification.
        #[prost(string, optional, tag="1")]
        pub name: ::core::option::Option<::prost::alloc::string::String>,
        #[prost(message, optional, tag="2")]
        pub value: ::core::option::Option<Value>,
    }
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Value {
        /// Hyper parameter value. Should match the type defined in the hyper
        /// parameter specification.
        #[prost(oneof="value::Type", tags="2, 3, 4, 5")]
        pub r#type: ::core::option::Option<value::Type>,
    }
    /// Nested message and enum types in `Value`.
    pub mod value {
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct CategoricalList {
            #[prost(string, repeated, tag="1")]
            pub values: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        }
        /// Hyper parameter value. Should match the type defined in the hyper
        /// parameter specification.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum Type {
            #[prost(string, tag="2")]
            Categorical(::prost::alloc::string::String),
            #[prost(int32, tag="3")]
            Integer(i32),
            #[prost(double, tag="4")]
            Real(f64),
            #[prost(message, tag="5")]
            CategoricalList(CategoricalList),
        }
    }
}
/// Definition of the type, possible values and default values of the generic
/// hyper parameters of a learner. Also contains some documentation (free text +
/// links).
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenericHyperParameterSpecification {
    /// Individual fields / hyper-parameters.
    /// Also contains the per-fields documentation.
    #[prost(map="string, message", tag="1")]
    pub fields: ::std::collections::HashMap<::prost::alloc::string::String, generic_hyper_parameter_specification::Value>,
    /// Documentation for the entire learner.
    #[prost(message, optional, tag="2")]
    pub documentation: ::core::option::Option<generic_hyper_parameter_specification::LearnerDocumentation>,
}
/// Nested message and enum types in `GenericHyperParameterSpecification`.
pub mod generic_hyper_parameter_specification {
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Value {
        #[prost(message, optional, tag="5")]
        pub documentation: ::core::option::Option<value::Documentation>,
        /// If set, this parameter exists conditionally on other parameter values.
        #[prost(message, optional, tag="7")]
        pub conditional: ::core::option::Option<Conditional>,
        /// If set, this parameter is mutually exclusive with other parameters.
        #[prost(message, optional, tag="8")]
        pub mutual_exclusive: ::core::option::Option<value::MutuallyExclusivityCondition>,
        #[prost(oneof="value::Type", tags="2, 3, 4, 6")]
        pub r#type: ::core::option::Option<value::Type>,
    }
    /// Nested message and enum types in `Value`.
    pub mod value {
        /// Categorical hyper parameter i.e. the hyper parameter takes a values
        /// from a set of possible values.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct Categorical {
            #[prost(string, repeated, tag="1")]
            pub possible_values: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
            #[prost(string, optional, tag="2")]
            pub default_value: ::core::option::Option<::prost::alloc::string::String>,
        }
        /// List of categorical values.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct CategoricalList {
        }
        /// Integer hyper parameter.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct Integer {
            #[prost(int32, optional, tag="1")]
            pub minimum: ::core::option::Option<i32>,
            #[prost(int32, optional, tag="2")]
            pub maximum: ::core::option::Option<i32>,
            #[prost(int32, optional, tag="3")]
            pub default_value: ::core::option::Option<i32>,
        }
        /// Real hyper parameter.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct Real {
            #[prost(double, optional, tag="1")]
            pub minimum: ::core::option::Option<f64>,
            #[prost(double, optional, tag="2")]
            pub maximum: ::core::option::Option<f64>,
            #[prost(double, optional, tag="3")]
            pub default_value: ::core::option::Option<f64>,
        }
        /// Links to the documentation of the hyper-parameter.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct Documentation {
            /// Path to the proto relative to YDF root directory.
            #[prost(string, optional, tag="1")]
            pub proto_path: ::core::option::Option<::prost::alloc::string::String>,
            /// Name of the proto field. If not specific, use "name" instead.
            #[prost(string, optional, tag="2")]
            pub proto_field: ::core::option::Option<::prost::alloc::string::String>,
            /// Free text description of the parameter.
            #[prost(string, optional, tag="3")]
            pub description: ::core::option::Option<::prost::alloc::string::String>,
            /// When a field is deprecated.
            #[prost(bool, optional, tag="8")]
            pub deprecated: ::core::option::Option<bool>,
        }
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct MutuallyExclusivityCondition {
            /// List of parameters this parameter is mutually exclusive with. Any
            /// parameter in this list must have this parameter in its
            /// `other_parameters` list.
            #[prost(string, repeated, tag="1")]
            pub other_parameters: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
            /// True if this parameter is the default parameter of a list of mutually
            /// exclusive parameters.
            #[prost(bool, optional, tag="2", default="false")]
            pub is_default: ::core::option::Option<bool>,
        }
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum Type {
            #[prost(message, tag="2")]
            Categorical(Categorical),
            #[prost(message, tag="3")]
            Integer(Integer),
            #[prost(message, tag="4")]
            Real(Real),
            #[prost(message, tag="6")]
            CategoricalList(CategoricalList),
        }
    }
    /// Conditional existence of a parameter.
    /// A parameter exist iff. the other parameter "control_field" satisfy
    /// "constraint".
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Conditional {
        /// Name of the control parameter.
        #[prost(string, optional, tag="1")]
        pub control_field: ::core::option::Option<::prost::alloc::string::String>,
        /// Constraint on the parent.
        #[prost(oneof="conditional::Constraint", tags="2")]
        pub constraint: ::core::option::Option<conditional::Constraint>,
    }
    /// Nested message and enum types in `Conditional`.
    pub mod conditional {
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct Categorical {
            #[prost(string, repeated, tag="1")]
            pub values: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        }
        /// Constraint on the parent.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum Constraint {
            /// One of the following values.
            #[prost(message, tag="2")]
            Categorical(Categorical),
        }
    }
    /// Documentation about the entire learner.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct LearnerDocumentation {
        /// Free text description of the learning algorithm.
        #[prost(string, optional, tag="1")]
        pub description: ::core::option::Option<::prost::alloc::string::String>,
    }
}
/// Set of hyper-parameter-sets aka. hyper-parameter search space.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct HyperParameterSpace {
    #[prost(message, repeated, tag="1")]
    pub fields: ::prost::alloc::vec::Vec<hyper_parameter_space::Field>,
}
/// Nested message and enum types in `HyperParameterSpace`.
pub mod hyper_parameter_space {
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Field {
        /// Name of the hyper parameter. Should match one of the generic hyper
        /// parameter of the model (use "GetGenericHyperParameterSpecification" for
        /// the list of generic hyper parameters).
        #[prost(string, optional, tag="1")]
        pub name: ::core::option::Option<::prost::alloc::string::String>,
        /// List of child fields.
        #[prost(message, repeated, tag="4")]
        pub children: ::prost::alloc::vec::Vec<Field>,
        /// Definition of the candidate values.
        #[prost(oneof="field::Type", tags="2")]
        pub r#type: ::core::option::Option<field::Type>,
        /// If this field has a parent field, then it is only activated if its
        /// parent's value is one of these.
        #[prost(oneof="field::MatchingParentValues", tags="3")]
        pub matching_parent_values: ::core::option::Option<field::MatchingParentValues>,
    }
    /// Nested message and enum types in `Field`.
    pub mod field {
        /// Definition of the candidate values.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum Type {
            #[prost(message, tag="2")]
            DiscreteCandidates(super::DiscreteCandidates),
        }
        /// If this field has a parent field, then it is only activated if its
        /// parent's value is one of these.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum MatchingParentValues {
            #[prost(message, tag="3")]
            ParentDiscreteValues(super::DiscreteCandidates),
        }
    }
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct DiscreteCandidates {
        #[prost(message, repeated, tag="1")]
        pub possible_values: ::prost::alloc::vec::Vec<super::generic_hyper_parameters::Value>,
        /// If set, "weights" has the same number of elements as "possible_values".
        /// "weights\[i\]" is the weight of this specific value for the optimizer.
        /// Different optimizers can use this weight differently.
        ///
        /// Random optimizer: Weight of the field during random sampling. If not
        ///    specified, all the hyper-parameter combinations have the same
        ///    probability of sampling. It means that a possible value with
        ///    conditional children will be more likely to be sampled.
        #[prost(double, repeated, packed="false", tag="2")]
        pub weights: ::prost::alloc::vec::Vec<f64>,
    }
}
/// Contains the same information as a model::AbstractModel (without the
/// data_spec field).
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AbstractModel {
    /// Name of the model. Should match one of the registered models in the
    /// :model_library.
    #[prost(string, optional, tag="1")]
    pub name: ::core::option::Option<::prost::alloc::string::String>,
    /// Task solved by the model e.g. classification, regression.
    #[prost(enumeration="Task", optional, tag="2")]
    pub task: ::core::option::Option<i32>,
    /// Index of the label column in the dataspec.
    #[prost(int32, optional, tag="3")]
    pub label_col_idx: ::core::option::Option<i32>,
    /// Training example weights.
    #[prost(message, optional, tag="4")]
    pub weights: ::core::option::Option<super::dataset::LinkedWeightDefinition>,
    /// List of indices (in the dataspec) of the model input features.
    #[prost(int32, repeated, packed="false", tag="5")]
    pub input_features: ::prost::alloc::vec::Vec<i32>,
    /// Index of the "grouping" attribute in the dataspec for ranking problems e.g.
    /// the query in a <query,document> ranking problem.
    #[prost(int32, optional, tag="6", default="-1")]
    pub ranking_group_col_idx: ::core::option::Option<i32>,
    /// Pre-computed variable importances (VI). The VIs of the model are composed
    /// of the pre-computed VIs (this field) and the "model specific VIs" (i.e.
    /// variable importance computed on the fly based on the models structure).
    #[prost(map="string, message", tag="7")]
    pub precomputed_variable_importances: ::std::collections::HashMap<::prost::alloc::string::String, VariableImportanceSet>,
    /// If true, the output of a task=CLASSIFICATION model is a probability and can
    /// be used accordingly (e.g. averaged, clamped to \[0,1\]). If false, the output
    /// of the task=CLASSIFICATION model might not be a probability.
    #[prost(bool, optional, tag="8", default="true")]
    pub classification_outputs_probabilities: ::core::option::Option<bool>,
    /// Index of the "treatment" attribute in the dataspec for uplift problems.
    #[prost(int32, optional, tag="9", default="-1")]
    pub uplift_treatment_col_idx: ::core::option::Option<i32>,
    #[prost(message, optional, tag="10")]
    pub metadata: ::core::option::Option<Metadata>,
    /// Logs of the automated hyper-parameter tuning of the model.
    #[prost(message, optional, tag="11")]
    pub hyperparameter_optimizer_logs: ::core::option::Option<HyperparametersOptimizerLogs>,
    /// Indicate if a model is pure for serving i.e. the model was tripped of all
    /// information not required for serving.
    #[prost(bool, optional, tag="12", default="false")]
    pub is_pure_model: ::core::option::Option<bool>,
}
/// Information about the model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Metadata {
    /// Owner of the model. Default to the user who ran the training code if
    /// available.
    #[prost(string, optional, tag="1")]
    pub owner: ::core::option::Option<::prost::alloc::string::String>,
    /// Unix Timestamp of the model training. Expressed in seconds.
    #[prost(int64, optional, tag="2")]
    pub created_date: ::core::option::Option<i64>,
    /// Unique identifier of the model.
    #[prost(uint64, optional, tag="3")]
    pub uid: ::core::option::Option<u64>,
    /// Framework used to create the model.
    #[prost(string, optional, tag="4")]
    pub framework: ::core::option::Option<::prost::alloc::string::String>,
}
/// Description of the importance of a given attribute. The semantic of
/// "importance" is variable.
///
/// Next ID: 3
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct VariableImportance {
    #[prost(int32, optional, tag="1")]
    pub attribute_idx: ::core::option::Option<i32>,
    #[prost(double, optional, tag="2")]
    pub importance: ::core::option::Option<f64>,
}
/// Next ID: 2
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct VariableImportanceSet {
    #[prost(message, repeated, tag="1")]
    pub variable_importances: ::prost::alloc::vec::Vec<VariableImportance>,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct HyperparametersOptimizerLogs {
    /// Optimization steps ordered chronologically by evaluation_time.
    #[prost(message, repeated, tag="1")]
    pub steps: ::prost::alloc::vec::Vec<hyperparameters_optimizer_logs::Step>,
    /// Domain of search for the hyper-parameters.
    #[prost(message, optional, tag="2")]
    pub space: ::core::option::Option<HyperParameterSpace>,
    ///   Registered key for the hyperparameter optimizer.
    #[prost(string, optional, tag="3")]
    pub hyperparameter_optimizer_key: ::core::option::Option<::prost::alloc::string::String>,
    /// The selected hyperparameters and its score.
    ///
    /// Note: It is possible that the best hyperparameters are not part of the
    /// "steps".
    #[prost(message, optional, tag="5")]
    pub best_hyperparameters: ::core::option::Option<GenericHyperParameters>,
    #[prost(float, optional, tag="4")]
    pub best_score: ::core::option::Option<f32>,
}
/// Nested message and enum types in `HyperparametersOptimizerLogs`.
pub mod hyperparameters_optimizer_logs {
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Step {
        /// Time, in seconds, relative to the start of the hyper-parameter tuning, of
        /// the consuption of the hyperparameters evaluation.
        #[prost(double, optional, tag="1")]
        pub evaluation_time: ::core::option::Option<f64>,
        /// Tested hyperparameters.
        #[prost(message, optional, tag="2")]
        pub hyperparameters: ::core::option::Option<super::GenericHyperParameters>,
        /// Score (the higher, the better) of the hyperparameters.
        /// A NaN value indicates that the hyperparameters are unfeasible.
        #[prost(float, optional, tag="3")]
        pub score: ::core::option::Option<f32>,
    }
}
/// Proto used to serialize / deserialize the model to / from string. See
/// "SerializeModel" and "DeserializeModel".
///
/// This message does not contains the entire model data.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SerializedModel {
    #[prost(message, optional, tag="1")]
    pub abstract_model: ::core::option::Option<AbstractModel>,
}
/// Modeling task.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum Task {
    Undefined = 0,
    Classification = 1,
    Regression = 2,
    /// In case of ranking, the label is expected to be between 0 and 4, and to
    /// have the NDCG semantic:
    /// 0: Completely unrelated.
    /// 4: Perfect match.
    Ranking = 3,
    /// Predicts the incremental impact of a treatment on a categorical outcome.
    /// See <https://en.wikipedia.org/wiki/Uplift_modelling.>
    CategoricalUplift = 4,
    /// Predicts the incremental impact of a treatment on a numerical outcome.
    /// See <https://en.wikipedia.org/wiki/Uplift_modelling.>
    NumericalUplift = 5,
    /// Predicts if an instance is similar to the majority of the training data or
    /// anomalous (a.k.a. an outlier). An anomaly detection prediction is a value
    /// between 0 and 1, where 0 indicates the possible most normal instance and 1
    /// indicates the most possible anomalous instance.
    AnomalyDetection = 6,
}
impl Task {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Task::Undefined => "UNDEFINED",
            Task::Classification => "CLASSIFICATION",
            Task::Regression => "REGRESSION",
            Task::Ranking => "RANKING",
            Task::CategoricalUplift => "CATEGORICAL_UPLIFT",
            Task::NumericalUplift => "NUMERICAL_UPLIFT",
            Task::AnomalyDetection => "ANOMALY_DETECTION",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "UNDEFINED" => Some(Self::Undefined),
            "CLASSIFICATION" => Some(Self::Classification),
            "REGRESSION" => Some(Self::Regression),
            "RANKING" => Some(Self::Ranking),
            "CATEGORICAL_UPLIFT" => Some(Self::CategoricalUplift),
            "NUMERICAL_UPLIFT" => Some(Self::NumericalUplift),
            "ANOMALY_DETECTION" => Some(Self::AnomalyDetection),
            _ => None,
        }
    }
}
/// Specification of the computing resources used to perform an action (e.g.
/// train a model, run a cross-validation, generate predictions). The deployment
/// configuration does not impact the results (e.g. learned model).
///
/// If not specified, more consumer will assume local computation with multiple
/// threads.
///
/// Next ID: 9
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeploymentConfig {
    /// Path to temporary directory available to the training algorithm.
    /// Currently cache_path is only used (and required) by the
    /// distributed algorithms or if "try_resume_training=True" (for the
    /// snapshots).
    ///
    /// In case of distributed training, the "cache_path" should be available by
    /// the manager and the workers (unless specified otherwise) -- so local
    /// machine/memory partition won't work.
    #[prost(string, optional, tag="1")]
    pub cache_path: ::core::option::Option<::prost::alloc::string::String>,
    /// Number of threads.
    #[prost(int32, optional, tag="2", default="6")]
    pub num_threads: ::core::option::Option<i32>,
    /// If true, try to resume an interrupted training using snapshots stored in
    /// the "cache_path". Not supported by all learning algorithms. Resuming
    /// training after changing the hyper-parameters might lead to failure when
    /// training is resumed.
    #[prost(bool, optional, tag="6", default="false")]
    pub try_resume_training: ::core::option::Option<bool>,
    /// Indicative number of seconds in between snapshots when
    /// "try_resume_training=True". Might be ignored by some algorithms.
    #[prost(int64, optional, tag="7", default="1800")]
    pub resume_training_snapshot_interval_seconds: ::core::option::Option<i64>,
    /// Number of threads to use for IO operations e.g. reading a dataset from
    /// disk. Increasing this value can speed-up IO operations when IO operations
    /// are either latency or cpu bounded.
    #[prost(int32, optional, tag="8", default="10")]
    pub num_io_threads: ::core::option::Option<i32>,
    /// Maximum number of snapshots to keep.
    #[prost(int32, optional, tag="9", default="3")]
    pub max_kept_snapshots: ::core::option::Option<i32>,
    /// Computation distribution engine.
    #[prost(oneof="deployment_config::Execution", tags="3, 5")]
    pub execution: ::core::option::Option<deployment_config::Execution>,
}
/// Nested message and enum types in `DeploymentConfig`.
pub mod deployment_config {
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Local {
    }
    /// Computation distribution engine.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Execution {
        /// Local execution.
        #[prost(message, tag="3")]
        Local(Local),
        /// Distribution using the Distribute interface.
        /// Note that the selected distribution strategy implementation (selected in
        /// "distribute") needs to be linked with the binary if you are using the C++
        /// API.
        #[prost(message, tag="5")]
        Distribute(super::super::distribute::Config),
    }
}
/// Training configuration.
/// Contains all the configuration for the training of a model e.g. label, input
/// features, hyper-parameters.
///
/// Next ID: 13
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TrainingConfig {
    /// Identifier of the learner e.g. "RANDOM_FOREST".
    /// The learner should be registered i.e. injected as a dependency to the
    /// binary. The list of available learners is available with
    /// "AllRegisteredModels()" in "model_library.h".
    #[prost(string, optional, tag="1")]
    pub learner: ::core::option::Option<::prost::alloc::string::String>,
    /// List of regular expressions over the dataset columns defining the input
    /// features of the model. If empty, all the columns (with the exception of the
    /// label and cv_group) will be added as input features.
    #[prost(string, repeated, tag="2")]
    pub features: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Label column.
    #[prost(string, optional, tag="3")]
    pub label: ::core::option::Option<::prost::alloc::string::String>,
    /// Name of the column used to split the dataset for in-training
    /// cross-validation i.e. all the records with the same "cv_group" value are in
    /// the same cross-validation fold. If not specified, examples are randomly
    /// assigned to train and test. This field is ignored by learner that do not
    /// run in-training cross-validation.
    #[prost(string, optional, tag="4")]
    pub cv_group: ::core::option::Option<::prost::alloc::string::String>,
    /// Task / problem solved by the model.
    #[prost(enumeration="Task", optional, tag="5", default="Classification")]
    pub task: ::core::option::Option<i32>,
    /// Weighting of the training examples. If not specified, the weight is
    /// assumed uniform.
    #[prost(message, optional, tag="6")]
    pub weight_definition: ::core::option::Option<super::dataset::WeightDefinition>,
    /// Random seed for the training of the model. Learners are expected to be
    /// deterministic by the random seed.
    #[prost(int64, optional, tag="7", default="123456")]
    pub random_seed: ::core::option::Option<i64>,
    /// Column identifying the groups in a ranking task.
    /// For example, in a document/query ranking problem, the "ranking_group" will
    /// be the query.
    ///
    /// The ranking column can be either a HASH or a CATEGORICAL. HASH is
    /// recommended. If CATEGORICAL, ensure dictionary is not pruned (i.e. minimum
    /// number of observations = 0 and maximum numbers of items = -1 => infinity).
    #[prost(string, optional, tag="8")]
    pub ranking_group: ::core::option::Option<::prost::alloc::string::String>,
    /// Maximum training duration of the training expressed in seconds. If the
    /// learner does not support constrained the training time, the training will
    /// fails immediately. Each learning algorithm is free to use this parameter as
    /// it see fit. Enabling maximum training duration makes the model training
    /// non-deterministic.
    #[prost(double, optional, tag="9")]
    pub maximum_training_duration_seconds: ::core::option::Option<f64>,
    /// Limits the trained model by memory usage. Different algorithms can enforce
    /// this limit differently. Serialized or compiled models are generally much
    /// smaller. This limit can be fussy: The final model can be slightly larger.
    #[prost(int64, optional, tag="11")]
    pub maximum_model_size_in_memory_in_bytes: ::core::option::Option<i64>,
    /// Categorical column identifying the treatment group in an uplift task.
    /// For example, whether a patient received a treatment in a study about the
    /// impact of a medication.
    ///
    /// Only binary treatments are currently supported.
    #[prost(string, optional, tag="12")]
    pub uplift_treatment: ::core::option::Option<::prost::alloc::string::String>,
    /// Metadata of the model.
    /// Non specified fields are automatically set. For example, if "metadata.date"
    /// is not set, it will be automatically set to the training date.
    #[prost(message, optional, tag="13")]
    pub metadata: ::core::option::Option<Metadata>,
    /// Clear the model from any information that is not required for model
    /// serving. This includes debugging, model interpretation and other meta-data.
    /// The size of the serialized model can be reduced significatively (50% model
    /// size reduction is common). This parameter has no impact on the quality,
    /// serving speed or RAM usage of model serving.
    #[prost(bool, optional, tag="14", default="false")]
    pub pure_serving_model: ::core::option::Option<bool>,
    /// Set of monotonic constraints between the model's input features and output.
    #[prost(message, repeated, tag="15")]
    pub monotonic_constraints: ::prost::alloc::vec::Vec<MonotonicConstraint>,
}
/// Resolution column string names into column indices.
/// The column indies are defined in a given dataspec e.g. If
/// dataspec.columns\[5\].name = "toto", then the column idx of "toto" is 5.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TrainingConfigLinking {
    /// Next ID: 10
    /// Input features of the models.
    #[prost(int32, repeated, tag="1")]
    pub features: ::prost::alloc::vec::Vec<i32>,
    /// Features of type NUMERICAL.
    #[prost(int32, repeated, tag="9")]
    pub numerical_features: ::prost::alloc::vec::Vec<i32>,
    /// Label column.
    #[prost(int32, optional, tag="2")]
    pub label: ::core::option::Option<i32>,
    /// Number categories of label (used for classification only).
    #[prost(int32, optional, tag="3")]
    pub num_label_classes: ::core::option::Option<i32>,
    /// Index of the column matching "cv_group" in the "TrainingConfig".
    #[prost(int32, optional, tag="4")]
    pub cv_group: ::core::option::Option<i32>,
    #[prost(message, optional, tag="7")]
    pub weight_definition: ::core::option::Option<super::dataset::LinkedWeightDefinition>,
    /// Index of the column matching "ranking_group" in the "TrainingConfig".
    #[prost(int32, optional, tag="8", default="-1")]
    pub ranking_group: ::core::option::Option<i32>,
    /// Index of the column matching "uplift_treatment" in the "TrainingConfig".
    #[prost(int32, optional, tag="12", default="-1")]
    pub uplift_treatment: ::core::option::Option<i32>,
    /// Data for specific dataset columns.
    /// This field is either empty, or contains exactly one value for each column
    /// in the dataset.
    #[prost(message, repeated, tag="13")]
    pub per_columns: ::prost::alloc::vec::Vec<PerColumn>,
}
/// Returns a list of hyper-parameter sets that outperforms the default
/// hyper-parameters (either generally or in specific scenarios). Like default
/// hyper-parameters, existing pre-defined hyper-parameters cannot change.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PredefinedHyperParameterTemplate {
    /// Name of the template. Should be unique for a given learning algorithm.
    #[prost(string, optional, tag="1")]
    pub name: ::core::option::Option<::prost::alloc::string::String>,
    /// Version of the template.
    #[prost(int32, optional, tag="2")]
    pub version: ::core::option::Option<i32>,
    /// Free text describing how this template was created.
    #[prost(string, optional, tag="3")]
    pub description: ::core::option::Option<::prost::alloc::string::String>,
    /// Effective hyper-parameters.
    #[prost(message, optional, tag="4")]
    pub parameters: ::core::option::Option<GenericHyperParameters>,
}
/// "Capabilities" of a learner.
///
/// Describe the capabilities/constraints/properties of a learner (all called
/// "capabilities"). Capabilities are non-restrictive i.e. enabling a capability
/// cannot restrict the domain of use of a learner/model (i.e. use "support_tpu"
/// instead of "require_tpu").
///
/// Using a learner with non-available capabilities raises an error.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct LearnerCapabilities {
    /// Does the learner support the "maximum_training_duration_seconds" parameter
    /// in the TrainingConfig.
    #[prost(bool, optional, tag="1", default="false")]
    pub support_max_training_duration: ::core::option::Option<bool>,
    /// The learner can resume training of the model from the "cache_path" given in
    /// the deployment configuration.
    #[prost(bool, optional, tag="2", default="false")]
    pub resume_training: ::core::option::Option<bool>,
    /// If true, the algorithm uses a validation dataset for training (e.g. for
    /// early stopping) and support for the validation dataset to be passed to the
    /// training method (with the "valid_dataset" or "typed_valid_path" argument).
    /// If the learning algorithm has the "use_validation_dataset" capability and
    /// no validation dataset is given to the training function, the learning
    /// algorithm will extract a validation dataset from the training dataset.
    #[prost(bool, optional, tag="3", default="false")]
    pub support_validation_dataset: ::core::option::Option<bool>,
    /// If true, the algorithm supports training datasets in the "partial cache
    /// dataset" format.
    #[prost(bool, optional, tag="4", default="false")]
    pub support_partial_cache_dataset_format: ::core::option::Option<bool>,
    /// If true, the algorithm supports training with a maximum model size
    /// (maximum_model_size_in_memory_in_bytes).
    #[prost(bool, optional, tag="5", default="false")]
    pub support_max_model_size_in_memory: ::core::option::Option<bool>,
    /// If true, the algorithm supports monotonic constraints over numerical
    /// features.
    #[prost(bool, optional, tag="6", default="false")]
    pub support_monotonic_constraints: ::core::option::Option<bool>,
    /// If true, the learner requires a label. If false, the learner does not
    /// require a label.
    #[prost(bool, optional, tag="7", default="true")]
    pub require_label: ::core::option::Option<bool>,
    /// If true, the learner supports custom losses.
    #[prost(bool, optional, tag="8", default="false")]
    pub support_custom_loss: ::core::option::Option<bool>,
}
/// Monotonic constraints between model's output and numerical input features.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MonotonicConstraint {
    /// Regular expressions over the input features.
    #[prost(string, optional, tag="1")]
    pub feature: ::core::option::Option<::prost::alloc::string::String>,
    #[prost(enumeration="monotonic_constraint::Direction", optional, tag="2", default="Increasing")]
    pub direction: ::core::option::Option<i32>,
}
/// Nested message and enum types in `MonotonicConstraint`.
pub mod monotonic_constraint {
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
    #[repr(i32)]
    pub enum Direction {
        /// Ensure the model output is monotonic increasing (non-strict) with the
        /// feature.
        Increasing = 0,
        /// Ensure the model output is monotonic decreasing (non-strict) with the
        /// feature.
        Decreasing = 1,
    }
    impl Direction {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Direction::Increasing => "INCREASING",
                Direction::Decreasing => "DECREASING",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "INCREASING" => Some(Self::Increasing),
                "DECREASING" => Some(Self::Decreasing),
                _ => None,
            }
        }
    }
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PerColumn {
    /// If set, the attribute has a monotonic constraint.
    /// Note: monotonic_constraint.feature might not be set.
    #[prost(message, optional, tag="1")]
    pub monotonic_constraint: ::core::option::Option<MonotonicConstraint>,
}
/// Generic prediction (prediction over a single example). Those are usually the
/// output of a ML model.
///
/// Optionally, it may contains the ground truth (e.g. the label value). When the
/// ground truth is present, such a "Prediction" proto can be used for evaluation
/// (see "metric.h").
///
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Prediction {
    #[prost(float, optional, tag="3", default="1")]
    pub weight: ::core::option::Option<f32>,
    /// Identifier about the example.
    #[prost(string, optional, tag="4")]
    pub example_key: ::core::option::Option<::prost::alloc::string::String>,
    #[prost(oneof="prediction::Type", tags="1, 2, 5, 6, 7")]
    pub r#type: ::core::option::Option<prediction::Type>,
}
/// Nested message and enum types in `Prediction`.
pub mod prediction {
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Classification {
        /// Predicted class as indexed in the dataspec.
        #[prost(int32, optional, tag="1")]
        pub value: ::core::option::Option<i32>,
        /// Predicted distribution over the possible classes. If specified, the
        /// following relation holds: "value == argmax_i(distribution\[i\])".
        #[prost(message, optional, tag="2")]
        pub distribution: ::core::option::Option<super::super::utils::IntegerDistributionFloat>,
        #[prost(int32, optional, tag="3")]
        pub ground_truth: ::core::option::Option<i32>,
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Regression {
        #[prost(float, optional, tag="1")]
        pub value: ::core::option::Option<f32>,
        #[prost(float, optional, tag="2")]
        pub ground_truth: ::core::option::Option<f32>,
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Ranking {
        /// Predicted relevance (the higher, the most likely to be selected).
        #[prost(float, optional, tag="1")]
        pub relevance: ::core::option::Option<f32>,
        #[prost(float, optional, tag="2")]
        pub ground_truth_relevance: ::core::option::Option<f32>,
        /// Group of the predictions. Predictions with a same group are competing.
        #[deprecated]
        #[prost(int32, optional, tag="3")]
        pub deprecated_group: ::core::option::Option<i32>,
        /// Group of the predictions. Can be a categorical or a hash value.
        #[prost(uint64, optional, tag="4")]
        pub group_id: ::core::option::Option<u64>,
    }
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Uplift {
        /// Predicted treatment effect.
        ///
        /// treatment_effect\[i\] is the effect of the "i+1"-th treatment (categorical
        /// value i+2) compared to the control group (0-th treatment; categorical
        /// value = 1). The treatment out-of-vocabulary item (value = 0) is not taken
        /// into account.
        #[prost(float, repeated, tag="1")]
        pub treatment_effect: ::prost::alloc::vec::Vec<f32>,
        /// Applied treatment. The control group is treatment = 1. Other treatments
        /// are >1.
        #[prost(int32, optional, tag="2")]
        pub treatment: ::core::option::Option<i32>,
        /// Outcome (with or without treatment).
        #[prost(oneof="uplift::OutcomeType", tags="3, 4")]
        pub outcome_type: ::core::option::Option<uplift::OutcomeType>,
    }
    /// Nested message and enum types in `Uplift`.
    pub mod uplift {
        /// Outcome (with or without treatment).
        #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
        pub enum OutcomeType {
            #[prost(int32, tag="3")]
            OutcomeCategorical(i32),
            #[prost(float, tag="4")]
            OutcomeNumerical(f32),
        }
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct AnomalyDetection {
        /// Anomaly score between 0 (normal) and 1 (anomaly).
        #[prost(float, optional, tag="1")]
        pub value: ::core::option::Option<f32>,
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Type {
        #[prost(message, tag="1")]
        Classification(Classification),
        #[prost(message, tag="2")]
        Regression(Regression),
        #[prost(message, tag="5")]
        Ranking(Ranking),
        #[prost(message, tag="6")]
        Uplift(Uplift),
        #[prost(message, tag="7")]
        AnomalyDetection(AnomalyDetection),
    }
}
// @@protoc_insertion_point(module)
